{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySTfQ_lroa6V"
   },
   "source": [
    "## Video Analysis Using Deep Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPsaI47Ql67k"
   },
   "source": [
    "Identify the character of TV series and calculates its screentime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aidioz5DeTyl"
   },
   "source": [
    "But In this project our aim is to transfer the image classification problem into a video analysis problem. We were trying to find a way to build a model that automatically identified specific people in a given video at a particular time interval.\n",
    "And it turned out that we can do it. \n",
    "\n",
    "So a small application of this project can be used as calculating a screen time of a person in the video.\n",
    "\n",
    "And this project is focusd on same application and we built a small POC (proof of concept ) to check how it works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMZ_pPYJeYYX"
   },
   "source": [
    "### The complete steps in bird eye view:\n",
    "1. Import and read the video, extract frames from it, and save them as images\n",
    "\n",
    "2. Label a few images for training the model \n",
    "\n",
    "3. Build our model on training data\n",
    "\n",
    "4. Make predictions for the remaining images\n",
    "\n",
    "5. Calculate the screen time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iXUBOGLfOExq"
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KyBSGtxONp5U",
    "outputId": "4921d770-bfc5-4292-fb32-84493d827726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytube\n",
      "  Downloading pytube-12.0.0-py3-none-any.whl (56 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |█████▉                          | 10 kB 19.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 20 kB 21.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 30 kB 11.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 40 kB 4.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 51 kB 4.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 56 kB 2.2 MB/s \n",
      "\u001b[?25hInstalling collected packages: pytube\n",
      "Successfully installed pytube-12.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qesJe2vBeqef"
   },
   "source": [
    "## Step – 1: Read the video, extract frames from it and save them as images\n",
    "\n",
    "\n",
    "We will first capture the video from the given directory using the `VideoCapture()` function, and then we’ll extract frames from the video and save them as an image using the `imwrite()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yU37mcSLN7na"
   },
   "outputs": [],
   "source": [
    "# Opens the Video file\n",
    "cap= cv2.VideoCapture('The Big Bang Theory - Science is dead.3gpp')\n",
    "i=0\n",
    "\n",
    "image_folder = 'img'\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret == False:\n",
    "        break\n",
    "    cv2.imwrite(image_folder+'/'+str(i)+'.jpg',frame)\n",
    "    i+=1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOXYtbM4e8f2"
   },
   "source": [
    "The video is now converted into individual frames. In this problem, there is only one class, either “Sheldon” or “No Sheldon”. To create a dataset, we need to separate images according to these two manually. For this, I have created a folder named “data” which is having two sub-folder “Sheldon” and “no_sheldon”. Then manually added images to these two sub-folders. After creating dataset we are ready to dive into the code and concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqLJa-TpfjMC"
   },
   "source": [
    "### Input Data and Preprocessing\n",
    "\n",
    "We are having data in the form of images. To prepare this data for input to our neural network, we need to do some preprocessing with the following steps:\n",
    "\n",
    "- Read all images one by one using openCV\n",
    "- Resize each image to (224, 224, 3) for the input to the model\n",
    "- Divide the data by 255 to make input features to neural network in the same range\n",
    "- Append to corresponding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6rayOw9kJU2",
    "outputId": "da19f2f0-5a69-437e-d3de-459577c9c967"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2330/2330 [00:15<00:00, 148.32it/s]\n",
      "100%|██████████| 1145/1145 [00:24<00:00, 46.07it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "img_path = '/content/drive/MyDrive/GG_DL_Project/data'\n",
    "\n",
    "class1_data = []\n",
    "class2_data = []\n",
    "for classes in os.listdir(img_path):\n",
    "        fin_path = os.path.join(img_path, classes)\n",
    "        for fin_classes in tqdm(os.listdir(fin_path)):\n",
    "            img = cv2.imread(os.path.join(fin_path, fin_classes))\n",
    "            #print(img.shape)\n",
    "            if img is None:\n",
    "              continue\n",
    "            else:\n",
    "              img = cv2.resize(img, (224,224))\n",
    "              img = img/255.\n",
    "              if classes == 'sheldon':\n",
    "                  class1_data.append(img)\n",
    "              else:\n",
    "                  class2_data.append(img)\n",
    "\n",
    "class1_data = np.array(class1_data)\n",
    "class2_data = np.array(class2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhDJJO9Cga_f"
   },
   "source": [
    "Here we will use VGG16 model trained on “imagenet” dataset. For this, we are using tensorflow high-level API Keras. With keras, you can directly import VGG16 model as shown in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kiKc1sfpIoA9",
    "outputId": "ba2eb92e-b0f7-488a-add1-e4b39053cdf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 0s 0us/step\n",
      "58900480/58889256 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "vgg_model = VGG16(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfY-F4G3goOM"
   },
   "source": [
    "VGG16 model trained with imagenet dataset predicts on lots of classes, but in this problem, we are only having one class, either “Sheldon” or “No Sheldon”. That’s why above we are using include_top = False, which signifies that we are not including fully connected layers from the VGG16 model. Now we will pass our input data to vgg_model and generate the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ntvRIMA9Iqxc"
   },
   "outputs": [],
   "source": [
    "vgg_class1 = vgg_model.predict(class1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6_iIXLczUtK1"
   },
   "outputs": [],
   "source": [
    "vgg_class2 = vgg_model.predict(class2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMbn-NLbg9O1"
   },
   "source": [
    "Since we are not including fully connected layers from VGG16 model, we need to create a model with some fully connected layers and an output layer with 1 class, either “Sheldon” or “No Sheldon”. Output features from VGG16 model will be having shape 7*7*512, which will be input shape for our model. Here I am also using dropout layer to make model less over-fit. Let’s see the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qg05_9GDIt61",
    "outputId": "cb93109b-1aab-436c-f561-e239fa3cd764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 25088)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              25691136  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,216,449\n",
      "Trainable params: 26,216,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(7*7*512,))\n",
    "\n",
    "dense1 = Dense(1024, activation = 'relu')(inputs)\n",
    "drop1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(512, activation = 'relu')(drop1)\n",
    "drop2 = Dropout(0.5)(dense2)\n",
    "outputs = Dense(1, activation = 'sigmoid')(drop2)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a6RFwvehW3T"
   },
   "source": [
    "Now we have input features from VGG16 model and our own network architecture defined above. Next thing is to train this neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "V0_q-wY9I5v7"
   },
   "outputs": [],
   "source": [
    "train_data = np.concatenate((vgg_class1[:3000], vgg_class2[:2000]), axis = 0)\n",
    "train_data = train_data.reshape(train_data.shape[0],7*7*512)\n",
    "\n",
    "valid_data = np.concatenate((vgg_class1[3000:], vgg_class2[2000:]), axis = 0)\n",
    "valid_data = valid_data.reshape(valid_data.shape[0],7*7*512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Zqq7vg1RrSS2"
   },
   "outputs": [],
   "source": [
    "train_label = np.array([0]*vgg_class1[:3000].shape[0] + [1]*vgg_class2[:2000].shape[0])\n",
    "valid_label = np.array([0]*vgg_class1[3000:].shape[0] + [1]*vgg_class2[2000:].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpAEL7n_hpbT"
   },
   "source": [
    "## Training the Network\n",
    "\n",
    "we will use stochastic gradient descent as an optimizer and binary cross-entropy as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Tzi7MxQsrVDn"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "model.compile(optimizer = 'sgd', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "filepath=\"best_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9-w3HHEh8Ih"
   },
   "source": [
    "I am using batch size of 64 and 10 epochs to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rknbNw_ErZcK",
    "outputId": "0aca2014-67be-46e3-febc-70ae29ea3a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "50/50 - 10s - loss: 0.6143 - accuracy: 0.7099 - val_loss: 0.0150 - val_accuracy: 1.0000 - 10s/epoch - 195ms/step\n",
      "Epoch 2/10\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "50/50 - 9s - loss: 0.3986 - accuracy: 0.8362 - val_loss: 0.3255 - val_accuracy: 0.9453 - 9s/epoch - 173ms/step\n",
      "Epoch 3/10\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "50/50 - 9s - loss: 0.2646 - accuracy: 0.8985 - val_loss: 0.0505 - val_accuracy: 0.9848 - 9s/epoch - 171ms/step\n",
      "Epoch 4/10\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "50/50 - 9s - loss: 0.2415 - accuracy: 0.8998 - val_loss: 0.1035 - val_accuracy: 0.9635 - 9s/epoch - 171ms/step\n",
      "Epoch 5/10\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "50/50 - 9s - loss: 0.1465 - accuracy: 0.9478 - val_loss: 2.8720 - val_accuracy: 0.0000e+00 - 9s/epoch - 172ms/step\n",
      "Epoch 6/10\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "50/50 - 9s - loss: 0.2211 - accuracy: 0.9243 - val_loss: 0.0174 - val_accuracy: 1.0000 - 9s/epoch - 173ms/step\n",
      "Epoch 7/10\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "50/50 - 9s - loss: 0.1587 - accuracy: 0.9475 - val_loss: 0.0301 - val_accuracy: 0.9787 - 9s/epoch - 174ms/step\n",
      "Epoch 8/10\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "50/50 - 9s - loss: 0.0643 - accuracy: 0.9857 - val_loss: 0.0102 - val_accuracy: 1.0000 - 9s/epoch - 176ms/step\n",
      "Epoch 9/10\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "50/50 - 9s - loss: 0.0547 - accuracy: 0.9879 - val_loss: 0.0318 - val_accuracy: 0.9757 - 9s/epoch - 173ms/step\n",
      "Epoch 10/10\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "50/50 - 9s - loss: 0.0409 - accuracy: 0.9914 - val_loss: 0.0355 - val_accuracy: 0.9757 - 9s/epoch - 173ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc6e0256d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_label, epochs = 10, batch_size = 64, validation_data = (valid_data, valid_label), verbose = 2, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qVKnVsaiErS"
   },
   "source": [
    "# Test video : collecting new video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xlxOvinPDG0W",
    "outputId": "45c300bd-6bf2-4ced-e263-bc1082e87d15"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/Sheldon On Teaching Women And then uses Google.3gpp'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test video\n",
    "\n",
    "\n",
    "from pytube import YouTube as yt\n",
    "\n",
    "video_link = 'https://www.youtube.com/watch?v=g_j869cfKDs'\n",
    "vid = yt(video_link)\n",
    "\n",
    "stream = vid.streams.first()\n",
    "stream.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuqIpM0ViK1i"
   },
   "source": [
    "## Extracting frames for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZS1xn5xrDWlg"
   },
   "outputs": [],
   "source": [
    "### Test video extraction \n",
    "\n",
    "\n",
    "# Opens the Video file\n",
    "cap= cv2.VideoCapture('Sheldon On Teaching Women And then uses Google.3gpp')\n",
    "i=0\n",
    "\n",
    "image_folder = '/content/drive/MyDrive/GG_DL_Project/test_data'\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret == False:\n",
    "        break\n",
    "    cv2.imwrite(image_folder+'/'+str(i)+'.jpg',frame)\n",
    "    i+=1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaK0xdTZidAh"
   },
   "source": [
    "## Calculating screen time :\n",
    "\n",
    "To test our trained model and calculate the screen time, I have downloaded another video clip from YouTube and extracted images. To calculate the screen time, first I have used the trained model to predict each image to find out which class it belongs, either “Sheldon” or “No Sheldon”. Since video is made up of 24 frames per second, we will count the number of frames which has been predicted for having “Sheldon” in it and then divide it by 24 to count the number of seconds “Sheldon” was on screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6X1frV4D1I2",
    "outputId": "fd830173-aa4c-4901-e51d-80d476c795e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2397/2397 [25:38<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sheldon_images = []\n",
    "no_sheldon_images = []\n",
    "\n",
    "test_path = '/content/drive/MyDrive/GG_DL_Project/test_data'\n",
    "\n",
    "for test in tqdm(os.listdir(test_path)):\n",
    "    test_img = cv2.imread(os.path.join(test_path, test))\n",
    "    if img is None:\n",
    "      continue\n",
    "    else:\n",
    "      test_img = cv2.resize(test_img, (224,224))\n",
    "      test_img = test_img/255.\n",
    "      test_img = np.expand_dims(test_img, 0)\n",
    "      pred_img = vgg_model.predict(test_img)\n",
    "      pred_feat = pred_img.reshape(1, 7*7*512)\n",
    "      out_class = model.predict(pred_feat)\n",
    "      if out_class < 0.5:\n",
    "          sheldon_images.append(out_class)\n",
    "      else:\n",
    "          no_sheldon_images.append(out_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cUlxFrKFdpv",
    "outputId": "af3a87d9-5ee4-41d2-fb31-9903ba6b3f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544\n"
     ]
    }
   ],
   "source": [
    "print(len(sheldon_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LawRRhX5jCyM"
   },
   "source": [
    "This test video clip is made up of 24 frames per second and number of images predicted for having “Sheldon” in it are 544. So the screen time for Sheldon will be 544/24 = 22 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CZUIY87jZ5M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GG_DL_Project_Sheldon.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
